# Instruction Tuning Large Language Models to Understand Electronic Health Records

**Authors:** Zhenbang Wu, Anant Dadu, Michael Nalls, Faraz Faghri, Jimeng Sun  

**Published at:** NeurIPS 2024 Datasets and Benchmarks Track (Spotlight)

[[üìëPaper](https://openreview.net/pdf?id=Dgy5WVgPd2)] [[ü™ßPoster](./poster.pdf)] [[üìΩÔ∏èSlides](./slides.pdf)]

<div style="text-align: center;">
    <img src="./images/logo.png" alt="Logo" height="200">
    <p>Generated by ChatGPT</p>
</div>


## Release
- [11 Dec 2024] A sample dataset with ~100 patients is added
- [11 Dec 2024] Code for dataset creation, model training, and response evaluation is released


## Contents
- [Core Dependencies](#core-dependencies)
- [Data Download](#data-download)
- [Model Download](#model-download)
- [Train](#train)
- [Evaluate](#evaluate)
- [Dataset Creation](#dataset-creation)
- [Citation](#citation)


## Core Dependencies  
```
python 3.9
torch 2.3.0
transformers 4.44.0
peft 0.10.0
```

## Data Download

<div style="text-align: center;">
    <img src="./images/dataset.png" alt="Dataset" width="600">
</div>

The **MIMIC-Instr** dataset will be hosted on [PhysioNet](https://physionet.org/) once the preparation and review process is complete.

A sample dataset generated from the [MIMIC-IV Demo](https://physionet.org/content/mimic-iv-demo/2.2/) database is available in the `sample_data` directory.

For early access to the full dataset, please reach out to Zhenbang Wu (zw12@illinois.edu) with your Physionet credentials. 


## Model Download

<div style="text-align: center;">
    <img src="./images/model.png" alt="model" width="600">
</div>

The pre-trained model checkpoints will be released via Hugging Face Model Hub soon.

For early access to the model checkpoints, please reach out to Zhenbang Wu (zw12@illinois.edu).


## Train

1. Prepare the instruction tuning data:
    - Option 1: download the MIMIC-Instr dataset from PhysioNet
    - Option 2: generate the instruction tuning data on your own following [Data Generation](#data-generation)

2. If you use the provided MIMIC-Instr dataset, run steps 1, 4, 7 in [Data Generation](#data-generation) to prepare the event sequence data and pre-compute the event embeddings.

3. Run the training script [train.py](src/train/train.py):
   - CMD: `sh src/train/train.sh`


## Evaluate

1. Generate the model response with [query_llemr.ipynb](src/eval/query_llemr.ipynb)

2. Compare the model response with the GPT-4 reference answer with [eval.ipynb](src/eval/eval.ipynb) (need OpenAI Azure service)

3. Summarize the results with [summary_eval.ipynb](src/eval/summary_eval.ipynb)

   
## Dataset Creation

1. Download the [MIMIC-IV](https://physionet.org/content/mimiciv/2.2/) in the `raw_data` directory

2. Download the [MIMIC-IV-Note](https://physionet.org/content/mimic-iv-note/2.2/) dataset in the `raw_data` directory

3. Run the following jupyter notebook to select the patient cohort: [01_cohort_selection.ipynb](src/preprocess/01_cohort_selection.ipynb)

4. Run the following jupyter notebooks to prepare the event sequence data:
   - 1. Extract events:
     - [02_event_static.ipynb](src/preprocess/02_event_static.ipynb) 
     - [02_event_hosp_diagnoses_icd.ipynb](src/preprocess/02_event_hosp_diagnoses_icd.ipynb)
     - [02_event_hosp_labevents.ipynb](src/preprocess/02_event_hosp_labevents.ipynb)
     - [02_event_hosp_microbiologyevents.ipynb](src/preprocess/02_event_hosp_microbiologyevents.ipynb)
     - [02_event_hosp_prescriptions.ipynb](src/preprocess/02_event_hosp_prescriptions.ipynb)
     - [02_event_hosp_transfers.ipynb](src/preprocess/02_event_hosp_transfers.ipynb)
     - [02_event_icu_chartevents.ipynb](src/preprocess/02_event_icu_chartevents.ipynb)
     - [02_event_icu_inputevents.ipynb](src/preprocess/02_event_icu_inputevents.ipynb)
     - [02_event_icu_outputevents.ipynb](src/preprocess/02_event_icu_outputevents.ipynb)
     - [02_event_icu_procedureevents.ipynb](src/preprocess/02_event_icu_procedureevents.ipynb)
   - 2. Merge events: [03_merge_events.ipynb](src/preprocess/03_merge_events.ipynb)

5. Run the following jupyter notebooks to generate the instruction tuning data:
   - Run this only if you want to generate the instruction tuning data on your own 
   - 1. Generate the schema alignment subset:
     - [04_template_qa_event.ipynb](src/preprocess/04_template_qa_event.ipynb)
     - [04_paraphrase_qa_event.ipynb](src/preprocess/04_paraphrase_qa_event.ipynb) (need OpenAI Azure service)
   - 2. Generate the instruction following subset:
     - [04_generate_qa_note.ipynb](src/preprocess/04_generate_qa_note.ipynb) (need OpenAI Azure service)

6. Split the data into train, validation, and test sets:
   - [05_data_split.ipynb](src/preprocess/05_data_split.ipynb)

7. Pre-compute the event embeddings with [06_precompute_event_embeddings.py](src/preprocess/06_precompute_event_embeddings.py):
    - CMD: `sh src/preprocess/precompute_event_embeddings.sh`

8. Generate the GPT-4 reference answer with [query_gpt4.ipynb](src/eval/query_gpt4.ipynb)


## Citation

If you find this work useful, please cite:
```
@inproceedings{
    wu2024instruction,
    title={Instruction Tuning Large Language Models to Understand Electronic Health Records},
    author={Zhenbang Wu and Anant Dadu and Michael Nalls and Faraz Faghri and Jimeng Sun},
    booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
    year={2024},
    url={https://openreview.net/forum?id=Dgy5WVgPd2}
}
```
